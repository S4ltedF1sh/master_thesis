{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3506ff-8a11-495e-978b-ada34277275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, logging\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from transformers import LongT5ForConditionalGeneration\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.data import Dataset\n",
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54787059-78dd-45a0-8cfb-ea980205c7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cb177139594c69b70a1d85e3df1905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba58167-4051-4e3d-acca-24f90303d8eb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3ca456-cbbe-4156-87dc-b803f01a495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_max_length = 512\n",
    "args_target_max_length = 184\n",
    "args_input_eos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e489702c-965b-4b1d-9c64-91e5b2ded132",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"discq/data/clinical_qg_one_sentence\"\n",
    "data = load_from_disk(data_path)\n",
    "\n",
    "raw_train_dataset = data['train']\n",
    "raw_eval_dataset = data['validation']\n",
    "\n",
    "train_df = pd.DataFrame(raw_train_dataset)\n",
    "eval_df = pd.DataFrame(raw_eval_dataset)\n",
    "\n",
    "PROMPTS = {\n",
    "    \"after_reading_what_question\": \"\"\"<<SYS>>\\nYou're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \\n<</SYS>>\\n\\nGiven the EMR: \\\"{text}\\\"\\nAfter reading the above EMR, what question do you have about \"{trigger}\"?\\nQuestion:\"\"\",\n",
    "}\n",
    "\n",
    "template = PROMPTS[\"after_reading_what_question\"]\n",
    "column_names = raw_eval_dataset.column_names\n",
    "padding = \"max_length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad74787-408c-45d1-b40d-64580d19fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(df):\n",
    "    txts = []\n",
    "    for idx, row in df.iterrows():\n",
    "        txt = \"<s>[INST] \" + template.format(text=row[\"text\"].strip(), trigger=row[\"trigger\"]) + \"\\n[/INST]\\n\" + row['question'] + \" </s>\"\n",
    "        txts.append(txt)\n",
    "    return pd.DataFrame.from_dict({'text': txts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f3fb08-a917-45f6-9871-68100b2af692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_data(train_df)\n",
    "eval_data = convert_data(eval_df)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_data)\n",
    "eval_dataset = datasets.Dataset.from_pandas(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a6f46d-2750-43df-8e38-6237bd9a25f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \n",
      "<</SYS>>\n",
      "\n",
      "Given the EMR: \"Mr. Hohlt is a 76 - year-old man with a history of hypertension , hyperlipidemia and prior myocardial infarction with a percutaneous transluminal coronary angioplasty of the right coronary artery in 2005 at the Cambridge Health Alliance .\"\n",
      "After reading the above EMR, what question do you have about \"prior myocardial infarction\"?\n",
      "Question:\n",
      "[/INST]\n",
      "When was this? </s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea99a14-6633-4548-8b09-77dc7821f32e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206376da-9891-4161-a83f-29e659613513",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "#lora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e3d506-3bdb-4d39-a2eb-88ae5ac38157",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"results/llama2/7b\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=1e-6,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=f\"results/logs\",\n",
    "    adam_epsilon=1e-3,\n",
    "    max_grad_norm=1.0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    gradient_accumulation_steps=16,\n",
    "    #bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e00c78b9-bfe9-42be-84e9-731ff0b9cfe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd96ac96f9c46e8a3f168a1ed28390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc35d8ae5d45f292a620149d5a1b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length = args_max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51544f39-21f0-4a00-82f2-2c0d8a013d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 35:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.005500</td>\n",
       "      <td>1.755797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>0.779789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.782800</td>\n",
       "      <td>0.658465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718500</td>\n",
       "      <td>0.630290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.625987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ukp-storage-1/pham/miniconda3/envs/thesis/lib/python3.11/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/ukp-storage-1/pham/miniconda3/envs/thesis/lib/python3.11/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/ukp-storage-1/pham/miniconda3/envs/thesis/lib/python3.11/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/ukp-storage-1/pham/miniconda3/envs/thesis/lib/python3.11/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/ukp-storage-1/pham/miniconda3/envs/thesis/lib/python3.11/site-packages/transformers/integrations/peft.py:389: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64bfcd-5207-4d9f-a3e5-3ab83b94d6bb",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022d2f73-aa93-462c-9124-edb60a7724be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32339922-061a-4127-a345-9c21d269e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "output_path = \"results/llama2_finetuned_generated.txt\"\n",
    "results = []\n",
    "baseline = pd.read_csv(\"baselines/baseline_with_context.csv\", sep='\\t')\n",
    "prompts = []\n",
    "sys_msg = \"<<SYS>>\\nYou're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \\n<</SYS>>\\n\\n\"\n",
    "\n",
    "\n",
    "for index, row in baseline.iterrows():\n",
    "    context = row[\"context\"].strip()\n",
    "    trigger = row[\"trigger\"]\n",
    "    context = context.strip().rstrip(\".\")\n",
    "    prompt = f\"<s>[INST] {sys_msg}Given the EMR: \\\"{context}\\\"\\nAfter reading the above EMR, what question do you have about \\\"{trigger}\\\"?\\nQuestion:\\n[/INST]\"\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1edcb51-37f7-496c-b78f-a497cfe9df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=500,\n",
    "    device=device,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75656e1f-7724-4af0-997a-bdc1e94779cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \n",
      "<</SYS>>\n",
      "\n",
      "Given the EMR: \"He was given Zosyn for empiric treatment . Gram stain and culture of the ascites failed to identify any organism . He was weaned from oxygen requirement and was continued on normal saline boluses and. intravenous albumin for treatment of prerenal azotemia . Somatostatin and midodrine were also given for this condition and he also received packed red blood cells . On postoperative day number ten , the patient returned to the floor with improved renal function and without oxygen requirement . The somatostatin and the midodrine were discontinued and the regimen of Lasix and spironolactone was started for diuresis \"\n",
      "After reading the above EMR, what question do you have about \"prerenal azotemia\"?\n",
      "Question:\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(prompts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1da65a3-a3f5-418d-b0a5-82319dd6d2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = text_gen(prompts, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee0fe86b-ccb7-4a3f-af3a-8f74b679e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \n",
      "<</SYS>>\n",
      "\n",
      "Given the EMR: \"He was given Unasyn for empiric antibiotic treatment . At the time , blood , urine and bile was sent for culture , which were all negative . A chest x-ray was also negative for any pneumonia . Postoperatively , Mr. Cordano also suffered from low urine output ranges between 25 to 30 cc per hour . Fractional excretion of sodium was less than 1% suggesting a prerenal etiology . The sodium creatinine peaked at 2.0 and the patient was treated with boluses of intravenous fluid and intravenous albumin infusion was also administered . He responded well to this regimen and made adequate urine output \"\n",
      "After reading the above EMR, what question do you have about \"low urine output ranges between 25 to 30 cc per hour .\"?\n",
      "Question:\n",
      "[/INST]\n",
      "How long did the patient have low urine output? \n"
     ]
    }
   ],
   "source": [
    "print(outputs[1][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb15e9a8-cacf-4a06-9460-817af019471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(outputs):\n",
    "    txt_file = f\"results/llama2/7b/generated/{i}.txt\"\n",
    "    with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "270764a7-c9ee-4b1c-9c68-3f7c04379c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \n",
      "<</SYS>>\n",
      "\n",
      "Given the EMR: \"T99.9  HR 70AF  BP 120/69  RR 20 O2sat 94%RA. Neuro A&Ox3 MAE , non focal exam. Pulm CTAB. CV irreg irreg , sternum stable incision CDI. Abdm soft , NT/ND/+BS. Ext warm 2+ edema. Pertinent Results :. 2019-06-25 04:10 PM. GLUCOSE - 97 UREA N - 44 * CREAT - 2.0 * SODIUM - 143 POTASSIUM - 4.1 CHLORIDE - 107 TOTAL CO2 - 27 ANION GAP - 13. 2019-06-25 04:10 PM. ALT(SGPT) - 19 AST(SGOT) - 19 LD(LDH) - 153 ALK PHOS - 58 TOT BILI - 0.4. 2019-06-25 04:10 PM. ALBUMIN - 4.3. 2019-06-25 04:10 PM. %HbA1c - 5.7. 2019-06-25 04:10 PM. TSH - 1.2\"\n",
      "After reading the above EMR, what question do you have about \"UREA N - 44 * CREAT - 2.0 *\"?\n",
      "Question:\n",
      "[/INST]  Given the EMR, what is the patient's urine output? \n"
     ]
    }
   ],
   "source": [
    "output = text_gen(prompts[1002])\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abcf60e5-1f72-499e-9444-6042eadbbb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You're a doctor and you were given the following EMR by another doctor. You have some questions for the doctor who gave that EMR to you to get more details about the patient. \n",
      "<</SYS>>\n",
      "\n",
      "Given the EMR: \"T99.9  HR 70AF  BP 120/69  RR 20 O2sat 94%RA. Neuro A&Ox3 MAE , non focal exam. Pulm CTAB. CV irreg irreg , sternum stable incision CDI. Abdm soft , NT/ND/+BS. Ext warm 2+ edema. Pertinent Results :. 2019-06-25 04:10 PM. GLUCOSE - 97 UREA N - 44 * CREAT - 2.0 * SODIUM - 143 POTASSIUM - 4.1 CHLORIDE - 107 TOTAL CO2 - 27 ANION GAP - 13. 2019-06-25 04:10 PM. ALT(SGPT) - 19 AST(SGOT) - 19 LD(LDH) - 153 ALK PHOS - 58 TOT BILI - 0.4. 2019-06-25 04:10 PM. ALBUMIN - 4.3. 2019-06-25 04:10 PM. %HbA1c - 5.7. 2019-06-25 04:10 PM. TSH - 1.2\"\n",
      "After reading the above EMR, what question do you have about \"UREA N - 44 * CREAT - 2.0 *\"?\n",
      "Question:\n",
      "[/INST]  Given the EMR, what is the patient's urine output? \n"
     ]
    }
   ],
   "source": [
    "print(outputs[1002][0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
